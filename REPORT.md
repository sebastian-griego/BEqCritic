# REPORT: ProofNetVerif selection quality

## What “good” means

Task: for each ProofNetVerif problem, pick **one** Lean statement from a set of autoformalization candidates.

Primary metric (proxy for BEq+/semantic equivalence): **Selected correct (%)**, i.e. the fraction of problems where
the chosen candidate has `correct=1` in `PAug/ProofNetVerif`.

Why a proxy: the paper’s BEq+ metric requires `lean-interact` plus a working Lean toolchain (downloads/builds Mathlib).
This report uses the dataset’s correctness labels for a fast, reproducible end-to-end comparison.

## Reproduce

From a clean checkout:

```bash
python -m venv .venv
source .venv/bin/activate
python -m pip install -U pip
python -m pip install -e .

# end-to-end: train → candidates → select → eval → A/B report
bash scripts/run_quickstart.sh
make results
cat results/results.md
```

All artifacts/logs are written under `runs/quickstart/` (see `runs/quickstart/*.log` and `runs/quickstart/timing.txt`).

## Results (ProofNetVerif test)

See `results/results.md` (generated by `make results`) for the current quickstart comparison table.

## Tuned run (L40 hybrid)

This run uses the local ProofNetVerif `train` split with a group split for eval (to avoid problem-id leakage),
then calibrates temperature on the `valid` grouped candidates. Selection uses a critic+BLEU hybrid with support
clustering, tuned on `valid` and evaluated once on `test`.

Results (test, `runs/l40_v2/ab_metrics.md`):
- selfbleu: 47.2% selected correct
- beqcritic_hybrid: 50.0% selected correct (+2.8), 74.2% given any-correct

Key config:
- train: `microsoft/deberta-v3-base`, 3 epochs, hard pos/neg sampling, symmetrized pairs, bf16
- selection: `--similarity hybrid --hybrid-alpha 0.5 --threshold 0.5 --cluster-mode support --support-frac 0.7`
  `--triangle-prune-margin 0.2 --tie-break medoid --cluster-rank size_then_cohesion`

Repro (from repo root, using 3 GPUs):

```bash
TOKENIZERS_PARALLELISM=false torchrun --nproc_per_node=3 -m beqcritic.train_beq_critic \
  --dataset hf_datasets/ProofNetVerif --split train --eval-size 0.1 \
  --pred-key lean4_prediction --ref-key lean4_formalization --label-key correct --problem-id-key id \
  --base-model microsoft/deberta-v3-base --output-dir runs/l40_v2/checkpoints/beqcritic_deberta_v3_base \
  --task-mix pred_vs_ref,cand_vs_cand --max-pos-per-problem 32 --max-neg-per-problem 64 \
  --cand-pos-sampling hard --cand-neg-sampling hard --symmetrize --epochs 3 --batch-size 8 --bf16 --seed 0

python -m beqcritic.make_grouped_candidates \
  --dataset hf_datasets/ProofNetVerif --split valid \
  --pred-key lean4_prediction --ref-key lean4_formalization --label-key correct --problem-id-key id \
  --output runs/l40_v2/proofnetverif_valid_candidates.jsonl

python -m beqcritic.make_grouped_candidates \
  --dataset hf_datasets/ProofNetVerif --split test \
  --pred-key lean4_prediction --ref-key lean4_formalization --label-key correct --problem-id-key id \
  --output runs/l40_v2/proofnetverif_test_candidates.jsonl

python -m beqcritic.calibrate_temperature \
  --model runs/l40_v2/checkpoints/beqcritic_deberta_v3_base \
  --input runs/l40_v2/proofnetverif_valid_candidates.jsonl --device cuda:0 --batch-size 64 --symmetrize

python -m beqcritic.score_and_select \
  --model runs/l40_v2/checkpoints/beqcritic_deberta_v3_base \
  --input runs/l40_v2/proofnetverif_test_candidates.jsonl \
  --output runs/l40_v2/proofnetverif_test_selection_beqcritic_hybrid.jsonl \
  --device cuda:0 --similarity hybrid --hybrid-alpha 0.5 \
  --threshold 0.5 --tie-break medoid --cluster-rank size_then_cohesion \
  --cluster-mode support --support-frac 0.7 --triangle-prune-margin 0.2
```

## BEq+ A/B (paper metric)

Clean A/B: fixed candidate pool, swap selector, score with BEq+.

Results (test, `runs/beqplus_ab_v1/ab_metrics_beqplus.md`):
- selfbleu: 45/178 (25.3%)
- beqcritic: 48/178 (27.0%) (+1.7)

Repro (from repo root):

```bash
python -m beqcritic.make_grouped_candidates \
  --dataset PAug/ProofNetVerif --split test \
  --pred-key lean4_prediction --ref-key lean4_formalization --label-key correct --problem-id-key id \
  --output runs/beqplus_ab_v1/proofnetverif_test_candidates.jsonl

python -m beqcritic.self_bleu_select \
  --input runs/beqplus_ab_v1/proofnetverif_test_candidates.jsonl \
  --output runs/beqplus_ab_v1/proofnetverif_test_selection_selfbleu.jsonl

python -m beqcritic.score_and_select \
  --model runs/l40_v2/checkpoints/beqcritic_deberta_v3_base \
  --input runs/beqplus_ab_v1/proofnetverif_test_candidates.jsonl \
  --output runs/beqplus_ab_v1/proofnetverif_test_selection_beqcritic.jsonl \
  --device cuda:0 --similarity critic \
  --threshold 0.5 --tie-break medoid --cluster-rank size_then_cohesion \
  --triangle-prune-margin 0.2

python -m beqcritic.paper_pipeline.beq_plus_eval \
  --dataset PAug/ProofNetVerif --split test \
  --selections-a runs/beqplus_ab_v1/proofnetverif_test_selection_selfbleu.jsonl --a-name selfbleu \
  --selections-b runs/beqplus_ab_v1/proofnetverif_test_selection_beqcritic.jsonl --b-name beqcritic \
  --lean-version v4.8.0 --timeout-s 60 \
  --output-jsonl runs/beqplus_ab_v1/beqplus_results.jsonl
```

## BEq+ oracle headroom

Oracle@pool (test candidates, `runs/beqplus_ab_v1/oracle_metrics.md`):
- oracle: 70/178 (39.3%)
- selfbleu: 45/178 (25.3%), gap 25
- beqcritic: 48/178 (27.0%), gap 22

This shows 22–25 problems where the pool contains a BEq+-correct candidate but consensus selection misses it.

## Reference-free verifier reranker (NL -> Lean)

Train a verifier on `(nl_statement, lean4_prediction)` with a pairwise ranking loss per problem id,
then select the max-scoring candidate per problem.

Results (test, BEq+, `runs/verifier_v1/ab_metrics_beqplus.md`):
- verifier: 59/178 (33.1%), +7.8 vs selfbleu, +6.1 vs beqcritic
- oracle gap for verifier: 11

Repro (from repo root; uses 2 GPUs because GPU1 was occupied):

```bash
TOKENIZERS_PARALLELISM=false CUDA_VISIBLE_DEVICES=0,2 torchrun --nproc_per_node=2 -m beqcritic.train_verifier \
  --dataset hf_datasets/ProofNetVerif --split train --eval-size 0.1 \
  --nl-key nl_statement --pred-key lean4_prediction --label-key correct --problem-id-key id \
  --base-model microsoft/deberta-v3-base \
  --output-dir runs/verifier_v1/checkpoints/nl_verifier_deberta_v3_base \
  --max-pairs-per-problem 64 --neg-sampling hard \
  --epochs 3 --batch-size 8 --bf16 --seed 0

python -m beqcritic.verifier_select \
  --model runs/verifier_v1/checkpoints/nl_verifier_deberta_v3_base \
  --dataset PAug/ProofNetVerif --split test \
  --input runs/beqplus_ab_v1/proofnetverif_test_candidates.jsonl \
  --output runs/verifier_v1/proofnetverif_test_selection_verifier.jsonl \
  --device cuda:0 --batch-size 64

python -m beqcritic.paper_pipeline.beq_plus_eval \
  --dataset PAug/ProofNetVerif --split test \
  --selections-a runs/beqplus_ab_v1/proofnetverif_test_selection_selfbleu.jsonl --a-name selfbleu \
  --selections-b runs/verifier_v1/proofnetverif_test_selection_verifier.jsonl --b-name verifier \
  --lean-version v4.8.0 --timeout-s 60 \
  --output-jsonl runs/verifier_v1/beqplus_results.jsonl
```

Verifier oracle-miss diagnostic (`runs/verifier_v1/oracle_miss_analysis.md`):
- 11 oracle-true / verifier-wrong problems
- best-correct rank under verifier: 8/11 within top-2, 9/11 within top-5, 11/11 within top-10
- mean score gap top1 vs best-correct: 2.72 (median 1.65)

This suggests a better decision rule on the top-k (e.g., cluster aggregation) should recover several misses.

Cluster-aggregate selector (top-M + BEqCritic clustering + mean(top2) score) did not change BEq+ accuracy:
- selfbleu vs clusteragg: `runs/verifier_v1/ab_metrics_beqplus_selfbleu_vs_clusteragg.md` (clusteragg 33.1%)

## Typecheck filtering before selection

Typecheck pre-pass using the same BEq+ wrapping logic (lean-interact + dataset headers) showed
all candidates in this pool already typecheck, so filtering is a no-op:

- Summary: `runs/typecheck_v1/typecheck_summary.md`
- Verifier top1 typechecks rate: 100% (`runs/typecheck_v1/verifier_typecheck_stats.md`)

BEq+ re-runs after typecheck-aware selection are unchanged:
- selfbleu vs beqcritic: `runs/typecheck_v1/ab_metrics_beqplus_selfbleu_vs_beqcritic.md`
- selfbleu vs verifier: `runs/typecheck_v1/ab_metrics_beqplus_selfbleu_vs_verifier.md`

Repro (from repo root):

```bash
python -m beqcritic.paper_pipeline.beq_plus_typecheck \
  --dataset PAug/ProofNetVerif --split test \
  --input runs/beqplus_ab_v1/proofnetverif_test_candidates.jsonl \
  --output runs/typecheck_v1/proofnetverif_test_candidates_typechecked.jsonl \
  --output-filtered runs/typecheck_v1/proofnetverif_test_candidates_typechecked_filtered.jsonl \
  --summary-md runs/typecheck_v1/typecheck_summary.md \
  --cache-path runs/typecheck_v1/typecheck_cache.json \
  --lean-version v4.8.0 --timeout-s 60

python -m beqcritic.self_bleu_select \
  --input runs/typecheck_v1/proofnetverif_test_candidates_typechecked_filtered.jsonl \
  --output runs/typecheck_v1/proofnetverif_test_selection_selfbleu.jsonl

python -m beqcritic.score_and_select \
  --model runs/l40_v2/checkpoints/beqcritic_deberta_v3_base \
  --input runs/typecheck_v1/proofnetverif_test_candidates_typechecked_filtered.jsonl \
  --output runs/typecheck_v1/proofnetverif_test_selection_beqcritic.jsonl \
  --device cuda:0 --similarity critic \
  --threshold 0.5 --tie-break medoid --cluster-rank size_then_cohesion \
  --triangle-prune-margin 0.2

python -m beqcritic.verifier_select \
  --model runs/verifier_v1/checkpoints/nl_verifier_deberta_v3_base \
  --dataset PAug/ProofNetVerif --split test \
  --input runs/typecheck_v1/proofnetverif_test_candidates_typechecked.jsonl \
  --output runs/typecheck_v1/proofnetverif_test_selection_verifier.jsonl \
  --device cuda:0 --batch-size 64 \
  --stats-md runs/typecheck_v1/verifier_typecheck_stats.md

python -m beqcritic.paper_pipeline.beq_plus_eval \
  --dataset PAug/ProofNetVerif --split test \
  --selections-a runs/typecheck_v1/proofnetverif_test_selection_selfbleu.jsonl --a-name selfbleu \
  --selections-b runs/typecheck_v1/proofnetverif_test_selection_beqcritic.jsonl --b-name beqcritic \
  --lean-version v4.8.0 --timeout-s 60 \
  --output-jsonl runs/typecheck_v1/beqplus_results_selfbleu_vs_beqcritic.jsonl

python -m beqcritic.paper_pipeline.beq_plus_eval \
  --dataset PAug/ProofNetVerif --split test \
  --selections-a runs/typecheck_v1/proofnetverif_test_selection_selfbleu.jsonl --a-name selfbleu \
  --selections-b runs/typecheck_v1/proofnetverif_test_selection_verifier.jsonl --b-name verifier \
  --lean-version v4.8.0 --timeout-s 60 \
  --output-jsonl runs/typecheck_v1/beqplus_results_selfbleu_vs_verifier.jsonl
```

Cost drivers:
- Both methods score all candidate pairs on this split: `∑_problems n(n-1)/2 = 6638`.
- BEqCritic’s per-pair cost is dominated by cross-encoder inference; Self-BLEU is dominated by tokenization + n-gram overlap.

## Failure modes observed

Representative examples (from `runs/quickstart`):

- False equivalence edges can create a larger *incorrect* cluster, causing size-based ranking to pick the wrong component.
  Example: `Rudin|exercise_1_18b` (BEqCritic picks an incorrect 3-node component while a 2-node correct component exists).
- Missed equivalence edges can fragment correct candidates into small components that lose against a larger noisy component.
- Surface-overlap baselines fail when the lexically “central” candidate is wrong (e.g., typeclass/structure mismatches),
  even if a small set of correct candidates is present.

## Recommendations

- **Default threshold:** start with `--threshold 0.5` and tune on a held-out dev split (or `--write-split-ids` + filter).
- **Triangle pruning / mutual-k:** keep `--triangle-prune-margin 0.2`; consider `--mutual-k 3` when you see “bridge” errors.
- **Calibration:** `beqcritic.calibrate_temperature` is cheap to run, but in this quickstart it learned `T≈1.0` (no change).
- **Scaling to larger candidate sets:** use `--critic-pair-mode knn --knn-k 10` to avoid O(n²) scoring.

## Notes on licensing

This repo does not include a `LICENSE` file; redistribution/derivative-use terms are unclear.

## Verifier training: fast-label gains vs BEq+ parity

### What changed
- Trained a listwise-only NL→Lean verifier on the ProofNetVerif train split, initialized from the verifier_v1 checkpoint.
- Evaluated selection with the fast label metric (dataset `correct`) on valid and test using the same fixed candidate pools.
- Ran BEq+ on test for the best listwise checkpoint, using the cached Lean project and resume-safe evaluation.

### BEq+ evaluation stability
- beqcritic/paper_pipeline/beq_plus_eval.py and beqcritic/paper_pipeline/beq_plus_oracle.py support:
  - --project-dir for a stable cached Lean project
  - --resume to skip completed ids and append+flush per record

### Fast label selection results (dataset `correct`, fixed pools)

**Valid (n = 183)**
- verifier_v1: 104/183 = 56.83% (oracle 114/183)
- listwise-train seed0: 107/183 = 58.47% (oracle 114/183)
- listwise-train seed1: 108/183 = 59.02% (oracle 114/183)
- Paired seed1 vs v1: wins 5, losses 1, ties_correct 103, ties_wrong 74

**Test (n = 178)**
- verifier_v1: 113/178 = 63.48% (oracle 120/178, gap 7)
- listwise-train seed0: 114/178 = 64.04% (gap 6)
- listwise-train seed1: 116/178 = 65.17% (gap 4)
- Paired seed1 vs v1: wins 3, losses 0, ties_correct 113, ties_wrong 62

**Ensemble check (seed0 + seed1)**
- ensemble: 114/178 = 64.04%
- ensemble vs seed1: wins 0, losses 2 (ensemble is worse)
- Winner for fast-label on test: listwise-train seed1

### BEq+ results on test (paper metric, fixed pool)

**Self-BLEU baseline vs verifier**
- selfbleu: 45/178 = 25.3%
- verifier_v1: 59/178 = 33.1%

**Self-BLEU baseline vs listwise-train seed1**
- selfbleu: 45/178 = 25.3%
- verifier_listwise_seed1: 59/178 = 33.1%

**Direct BEq+ comparison: verifier_v1 vs listwise-train seed1**
- No disagreements across 178 problems
- wins 0, losses 0, ties_correct 59, ties_wrong 119
- Conclusion: on this pool, BEq+ is insensitive to the verifier changes so far

### Takeaway
- Listwise training improves label-based verification on valid and test.
- Those improvements do not translate to BEq+ on this fixed ProofNetVerif test candidate pool.

### Next direction
1. BEq+-distilled verifier: generate BEq+ labels for candidates (resume-safe, cached Lean project), then train the verifier directly to predict BEq+ success with a listwise objective.
2. Raise BEq+ oracle: increase or diversify the candidate pool (more generations or additional generators), then rerun selection. The current pool caps BEq+ at Oracle@pool.
