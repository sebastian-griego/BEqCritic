# REPORT: ProofNetVerif selection quality

## What “good” means

Task: for each ProofNetVerif problem, pick **one** Lean statement from a set of autoformalization candidates.

Primary metric (proxy for BEq+/semantic equivalence): **Selected correct (%)**, i.e. the fraction of problems where
the chosen candidate has `correct=1` in `PAug/ProofNetVerif`.

Why a proxy: the paper’s BEq+ metric requires `lean-interact` plus a working Lean toolchain (downloads/builds Mathlib).
This report uses the dataset’s correctness labels for a fast, reproducible end-to-end comparison.

## Reproduce

From a clean checkout:

```bash
python -m venv .venv
source .venv/bin/activate
python -m pip install -U pip
python -m pip install -e .

# end-to-end: train → candidates → select → eval → A/B report
bash scripts/run_quickstart.sh
make results
cat results/results.md
```

All artifacts/logs are written under `runs/quickstart/` (see `runs/quickstart/*.log` and `runs/quickstart/timing.txt`).

## Results (ProofNetVerif test)

See `results/results.md` (generated by `make results`) for the current quickstart comparison table.

## Tuned run (L40 hybrid)

This run uses the local ProofNetVerif `train` split with a group split for eval (to avoid problem-id leakage),
then calibrates temperature on the `valid` grouped candidates. Selection uses a critic+BLEU hybrid with support
clustering, tuned on `valid` and evaluated once on `test`.

Results (test, `runs/l40_v2/ab_metrics.md`):
- selfbleu: 47.2% selected correct
- beqcritic_hybrid: 50.0% selected correct (+2.8), 74.2% given any-correct

Key config:
- train: `microsoft/deberta-v3-base`, 3 epochs, hard pos/neg sampling, symmetrized pairs, bf16
- selection: `--similarity hybrid --hybrid-alpha 0.5 --threshold 0.5 --cluster-mode support --support-frac 0.7`
  `--triangle-prune-margin 0.2 --tie-break medoid --cluster-rank size_then_cohesion`

Repro (from repo root, using 3 GPUs):

```bash
TOKENIZERS_PARALLELISM=false torchrun --nproc_per_node=3 -m beqcritic.train_beq_critic \
  --dataset hf_datasets/ProofNetVerif --split train --eval-size 0.1 \
  --pred-key lean4_prediction --ref-key lean4_formalization --label-key correct --problem-id-key id \
  --base-model microsoft/deberta-v3-base --output-dir runs/l40_v2/checkpoints/beqcritic_deberta_v3_base \
  --task-mix pred_vs_ref,cand_vs_cand --max-pos-per-problem 32 --max-neg-per-problem 64 \
  --cand-pos-sampling hard --cand-neg-sampling hard --symmetrize --epochs 3 --batch-size 8 --bf16 --seed 0

python -m beqcritic.make_grouped_candidates \
  --dataset hf_datasets/ProofNetVerif --split valid \
  --pred-key lean4_prediction --ref-key lean4_formalization --label-key correct --problem-id-key id \
  --output runs/l40_v2/proofnetverif_valid_candidates.jsonl

python -m beqcritic.make_grouped_candidates \
  --dataset hf_datasets/ProofNetVerif --split test \
  --pred-key lean4_prediction --ref-key lean4_formalization --label-key correct --problem-id-key id \
  --output runs/l40_v2/proofnetverif_test_candidates.jsonl

python -m beqcritic.calibrate_temperature \
  --model runs/l40_v2/checkpoints/beqcritic_deberta_v3_base \
  --input runs/l40_v2/proofnetverif_valid_candidates.jsonl --device cuda:0 --batch-size 64 --symmetrize

python -m beqcritic.score_and_select \
  --model runs/l40_v2/checkpoints/beqcritic_deberta_v3_base \
  --input runs/l40_v2/proofnetverif_test_candidates.jsonl \
  --output runs/l40_v2/proofnetverif_test_selection_beqcritic_hybrid.jsonl \
  --device cuda:0 --similarity hybrid --hybrid-alpha 0.5 \
  --threshold 0.5 --tie-break medoid --cluster-rank size_then_cohesion \
  --cluster-mode support --support-frac 0.7 --triangle-prune-margin 0.2
```

## BEq+ A/B (paper metric)

Clean A/B: fixed candidate pool, swap selector, score with BEq+.

Results (test, `runs/beqplus_ab_v1/ab_metrics_beqplus.md`):
- selfbleu: 45/178 (25.3%)
- beqcritic: 48/178 (27.0%) (+1.7)

Repro (from repo root):

```bash
python -m beqcritic.make_grouped_candidates \
  --dataset PAug/ProofNetVerif --split test \
  --pred-key lean4_prediction --ref-key lean4_formalization --label-key correct --problem-id-key id \
  --output runs/beqplus_ab_v1/proofnetverif_test_candidates.jsonl

python -m beqcritic.self_bleu_select \
  --input runs/beqplus_ab_v1/proofnetverif_test_candidates.jsonl \
  --output runs/beqplus_ab_v1/proofnetverif_test_selection_selfbleu.jsonl

python -m beqcritic.score_and_select \
  --model runs/l40_v2/checkpoints/beqcritic_deberta_v3_base \
  --input runs/beqplus_ab_v1/proofnetverif_test_candidates.jsonl \
  --output runs/beqplus_ab_v1/proofnetverif_test_selection_beqcritic.jsonl \
  --device cuda:0 --similarity critic \
  --threshold 0.5 --tie-break medoid --cluster-rank size_then_cohesion \
  --triangle-prune-margin 0.2

python -m beqcritic.paper_pipeline.beq_plus_eval \
  --dataset PAug/ProofNetVerif --split test \
  --selections-a runs/beqplus_ab_v1/proofnetverif_test_selection_selfbleu.jsonl --a-name selfbleu \
  --selections-b runs/beqplus_ab_v1/proofnetverif_test_selection_beqcritic.jsonl --b-name beqcritic \
  --lean-version v4.8.0 --timeout-s 60 \
  --output-jsonl runs/beqplus_ab_v1/beqplus_results.jsonl
```

Cost drivers:
- Both methods score all candidate pairs on this split: `∑_problems n(n-1)/2 = 6638`.
- BEqCritic’s per-pair cost is dominated by cross-encoder inference; Self-BLEU is dominated by tokenization + n-gram overlap.

## Failure modes observed

Representative examples (from `runs/quickstart`):

- False equivalence edges can create a larger *incorrect* cluster, causing size-based ranking to pick the wrong component.
  Example: `Rudin|exercise_1_18b` (BEqCritic picks an incorrect 3-node component while a 2-node correct component exists).
- Missed equivalence edges can fragment correct candidates into small components that lose against a larger noisy component.
- Surface-overlap baselines fail when the lexically “central” candidate is wrong (e.g., typeclass/structure mismatches),
  even if a small set of correct candidates is present.

## Recommendations

- **Default threshold:** start with `--threshold 0.5` and tune on a held-out dev split (or `--write-split-ids` + filter).
- **Triangle pruning / mutual-k:** keep `--triangle-prune-margin 0.2`; consider `--mutual-k 3` when you see “bridge” errors.
- **Calibration:** `beqcritic.calibrate_temperature` is cheap to run, but in this quickstart it learned `T≈1.0` (no change).
- **Scaling to larger candidate sets:** use `--critic-pair-mode knn --knn-k 10` to avoid O(n²) scoring.

## Notes on licensing

This repo does not include a `LICENSE` file; redistribution/derivative-use terms are unclear.
